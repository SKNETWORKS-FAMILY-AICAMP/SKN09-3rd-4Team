{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install fitz frontend pymupdf pypdf dotenv langchain torch langchain_community transformers datasets\n",
    "!pip install \"numpy<2\"\n",
    "!pip install --upgrade pybind11\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 141)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# path = \"C:\\\\KIMUJUNG\\\\team_project\\\\team_project3\\\\pdf_folder\"\n",
    "path = \"pdf_folder\"\n",
    "file_list = os.listdir(path)\n",
    "car_list = []\n",
    "short_list = []\n",
    "long_hurt = []\n",
    "long_save = []\n",
    "long_etc = []\n",
    "for i in file_list:\n",
    "    if i[:2] in ['개인','업무','영업','이륜','외화','농기','기타','모터','공동']:\n",
    "        car_list.append(i)\n",
    "    elif i[:2] == '일반':\n",
    "        short_list.append(i)\n",
    "    elif i[:2] == \"상해\":\n",
    "        long_hurt.append(i)\n",
    "    elif i[:2] in ['저축','연금']:\n",
    "        long_save.append(i)\n",
    "    elif i[:2] in ['재물','제도']:\n",
    "        long_etc.append(i)\n",
    "    elif i[:5] in [\"운전면허교\",\"운전자_l\"]:\n",
    "        car_list.append(i)\n",
    "    elif i[:5] == \"운전자_(\":\n",
    "        long_etc.append(i)\n",
    "\n",
    "len(file_list), len(car_list)+len(short_list)+len(long_hurt)+len(long_save)+len(long_etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 55, 41, 3, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_list),len(short_list),len(long_hurt),len(long_save),len(long_etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwj/team_proj/faiss_2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "# 1️⃣ GPU 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동차 보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 공동인수_영업용 자동차보험(공동).pdf 처리 완료\n",
      "✅ 이륜차_let way 이륜자동차보험.pdf 처리 완료\n",
      "✅ 개인용_let click 개인용자동차보험(인터넷).pdf 처리 완료\n",
      "✅ 업무용_let way 업무용법인소유자동차보험.pdf 처리 완료\n",
      "✅ 기타_let way 자동차취급업자종합보험.pdf 처리 완료\n",
      "✅ 운전면허교습생_let way 운전면허교습생자동차보험.pdf 처리 완료\n",
      "✅ 공동인수_업무용 자동차보험(공동).pdf 처리 완료\n",
      "✅ 개인용_let way 개인용자동차보험.pdf 처리 완료\n",
      "✅ 업무용_let way 업무용개인소유자동차보험.pdf 처리 완료\n",
      "✅ 업무용_let click 업무용법인소유자동차보험(인터넷).pdf 처리 완료\n",
      "✅ 농기계_let way 농기계보험.pdf 처리 완료\n",
      "✅ 운전자_let way 운전자보험.pdf 처리 완료\n",
      "✅ 공동인수_개인용 자동차보험(공동).pdf 처리 완료\n",
      "✅ 기타_let way 배달플랫폼자동차보험.pdf 처리 완료\n",
      "✅ 업무용_let click 업무용개인소유자동차보험(TM).pdf 처리 완료\n",
      "✅ 영업용_let click 영업용자동차보험(TM).pdf 처리 완료\n",
      "✅ 업무용_let click 업무용개인소유자동차보험(인터넷).pdf 처리 완료\n",
      "✅ 기타_CREW 원데이자동차보험.pdf 처리 완료\n",
      "✅ 개인용_let click 개인용자동차보험(TM).pdf 처리 완료\n",
      "✅ 업무용_let click 업무용법인소유자동차보험(TM).pdf 처리 완료\n",
      "✅ 공동인수_이륜자동차보험(공동).pdf 처리 완료\n",
      "✅ 영업용_let way 영업용자동차보험.pdf 처리 완료\n",
      "✅ 기타_let click 배달플랫폼자동차보험(인터넷).pdf 처리 완료\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_lotte_car.bin\"\n",
    "metadata_path = \"./documents_lotte_car.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in car_list:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDF의 메타데이터 추출\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "    \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반 보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_lotte_short.bin\"\n",
    "metadata_path = \"./documents_lotte_short.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in short_list:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDF의 메타데이터 추출\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "# 1️⃣3️⃣ FAISS 인덱스 저장\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장기 - 상해, 질병"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_lotte_long_hurt.bin\"\n",
    "metadata_path = \"./documents_lotte_long_hurt.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_hurt:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDF의 메타데이터 추출\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "        \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장기 - 연금, 저축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 연금보험_let care 연금저축손해보험Ⅰ(2401).pdf 처리 완료\n",
      "✅ 연금보험_let care 연금저축손해보험Ⅱ(2401).pdf 처리 완료\n",
      "✅ 연금보험_let care 연금저축손해보험(계약이전)(2401).pdf 처리 완료\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_lotte_long_save.bin\"\n",
    "metadata_path = \"./documents_lotte_long_save.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_save:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDF의 메타데이터 추출\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)    \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장기 - 기타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_lotte_long_etc.bin\"\n",
    "metadata_path = \"./documents_lotte_long_etc.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_etc:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDF의 메타데이터 추출\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)    \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1️⃣ 저장된 FAISS 인덱스 로드\n",
    "index = faiss.read_index(\"./faiss_index.bin\")\n",
    "\n",
    "# 2️⃣ 문서 정보 로드\n",
    "with open(\"./documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# 3️⃣ 검색 수행\n",
    "# query = \" 보험금의 종류 및 한도에 대해 설명해줘?\"\n",
    "query = \"자동차 의무보험 미가입에 따른 불이익을 알려줘\"\n",
    "query_embedding = get_embeddings([query])[0]  # 쿼리 임베딩\n",
    "query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "\n",
    "D, I = index.search(query_embedding, k=10)  # 가장 유사한 5개 검색\n",
    "context = []\n",
    "\n",
    "# 4️⃣ 검색 결과 출력\n",
    "for idx in I[0]:\n",
    "    context.append(documents[idx].page_content)\n",
    "    print(f\"🔹 문서 {idx}: {documents[idx].page_content[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (faiss_2)",
   "language": "python",
   "name": "faiss_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

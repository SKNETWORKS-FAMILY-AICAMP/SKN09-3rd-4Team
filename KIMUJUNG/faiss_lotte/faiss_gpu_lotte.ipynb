{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install fitz frontend pymupdf pypdf dotenv langchain torch langchain_community transformers datasets\n",
    "!pip install \"numpy<2\"\n",
    "!pip install --upgrade pybind11\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 141)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# path = \"C:\\\\KIMUJUNG\\\\team_project\\\\team_project3\\\\pdf_folder\"\n",
    "path = \"pdf_folder\"\n",
    "file_list = os.listdir(path)\n",
    "car_list = []\n",
    "short_list = []\n",
    "long_hurt = []\n",
    "long_save = []\n",
    "long_etc = []\n",
    "for i in file_list:\n",
    "    if i[:2] in ['ê°œì¸','ì—…ë¬´','ì˜ì—…','ì´ë¥œ','ì™¸í™”','ë†ê¸°','ê¸°íƒ€','ëª¨í„°','ê³µë™']:\n",
    "        car_list.append(i)\n",
    "    elif i[:2] == 'ì¼ë°˜':\n",
    "        short_list.append(i)\n",
    "    elif i[:2] == \"ìƒí•´\":\n",
    "        long_hurt.append(i)\n",
    "    elif i[:2] in ['ì €ì¶•','ì—°ê¸ˆ']:\n",
    "        long_save.append(i)\n",
    "    elif i[:2] in ['ìž¬ë¬¼','ì œë„']:\n",
    "        long_etc.append(i)\n",
    "    elif i[:5] in [\"ìš´ì „ë©´í—ˆêµ\",\"ìš´ì „ìž_l\"]:\n",
    "        car_list.append(i)\n",
    "    elif i[:5] == \"ìš´ì „ìž_(\":\n",
    "        long_etc.append(i)\n",
    "\n",
    "len(file_list), len(car_list)+len(short_list)+len(long_hurt)+len(long_save)+len(long_etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 55, 41, 3, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car_list),len(short_list),len(long_hurt),len(long_save),len(long_etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwj/team_proj/faiss_2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "# 1ï¸âƒ£ GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìžë™ì°¨ ë³´í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê³µë™ì¸ìˆ˜_ì˜ì—…ìš© ìžë™ì°¨ë³´í—˜(ê³µë™).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì´ë¥œì°¨_let way ì´ë¥œìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê°œì¸ìš©_let click ê°œì¸ìš©ìžë™ì°¨ë³´í—˜(ì¸í„°ë„·).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—…ë¬´ìš©_let way ì—…ë¬´ìš©ë²•ì¸ì†Œìœ ìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê¸°íƒ€_let way ìžë™ì°¨ì·¨ê¸‰ì—…ìžì¢…í•©ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ìš´ì „ë©´í—ˆêµìŠµìƒ_let way ìš´ì „ë©´í—ˆêµìŠµìƒìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê³µë™ì¸ìˆ˜_ì—…ë¬´ìš© ìžë™ì°¨ë³´í—˜(ê³µë™).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê°œì¸ìš©_let way ê°œì¸ìš©ìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—…ë¬´ìš©_let way ì—…ë¬´ìš©ê°œì¸ì†Œìœ ìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—…ë¬´ìš©_let click ì—…ë¬´ìš©ë²•ì¸ì†Œìœ ìžë™ì°¨ë³´í—˜(ì¸í„°ë„·).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ë†ê¸°ê³„_let way ë†ê¸°ê³„ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ìš´ì „ìž_let way ìš´ì „ìžë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê³µë™ì¸ìˆ˜_ê°œì¸ìš© ìžë™ì°¨ë³´í—˜(ê³µë™).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê¸°íƒ€_let way ë°°ë‹¬í”Œëž«í¼ìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—…ë¬´ìš©_let click ì—…ë¬´ìš©ê°œì¸ì†Œìœ ìžë™ì°¨ë³´í—˜(TM).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì˜ì—…ìš©_let click ì˜ì—…ìš©ìžë™ì°¨ë³´í—˜(TM).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—…ë¬´ìš©_let click ì—…ë¬´ìš©ê°œì¸ì†Œìœ ìžë™ì°¨ë³´í—˜(ì¸í„°ë„·).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê¸°íƒ€_CREW ì›ë°ì´ìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê°œì¸ìš©_let click ê°œì¸ìš©ìžë™ì°¨ë³´í—˜(TM).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—…ë¬´ìš©_let click ì—…ë¬´ìš©ë²•ì¸ì†Œìœ ìžë™ì°¨ë³´í—˜(TM).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê³µë™ì¸ìˆ˜_ì´ë¥œìžë™ì°¨ë³´í—˜(ê³µë™).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì˜ì—…ìš©_let way ì˜ì—…ìš©ìžë™ì°¨ë³´í—˜.pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ê¸°íƒ€_let click ë°°ë‹¬í”Œëž«í¼ìžë™ì°¨ë³´í—˜(ì¸í„°ë„·).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "ðŸŽ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ìž¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# 2ï¸âƒ£ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (GPUë¡œ ì´ë™)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3ï¸âƒ£ ìž„ë² ë”© ìƒì„± í•¨ìˆ˜\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # íŽ˜ì´ì§€ ë²ˆí˜¸ ì œê±°\n",
    "    text = re.sub(r\"(ì œìž‘ì¼|ì£¼ì†Œ|QRì½”ë“œ|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # ëª©ì°¨ ì œê±°\n",
    "    text = re.sub(r\"ê¸ˆì†Œë²•|ë²•ë ¹\", \"\", text)  # ë²•ì  ê³µì§€ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # ê³µë°± ì •ë¦¬\n",
    "    return text.strip()\n",
    "\n",
    "# 4ï¸âƒ£ ì €ìž¥ ê²½ë¡œ ì„¤ì •\n",
    "faiss_index_path = \"./faiss_index_lotte_car.bin\"\n",
    "metadata_path = \"./documents_lotte_car.pkl\"\n",
    "\n",
    "# 6ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„±\n",
    "res = faiss.StandardGpuResources()  # GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in car_list:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDFì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # âœ… ì „ì²´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ì €ìž¥\n",
    "\n",
    "    # 7ï¸âƒ£ í…ìŠ¤íŠ¸ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°\n",
    "    total_text = \"\\n\".join(all_texts)  # âœ… í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ ë³‘í•©\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # âœ… ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n",
    "\n",
    "    # 8ï¸âƒ£ ìƒˆë¡œìš´ íŽ˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€í•˜ë©° Document í˜•ì‹ ë³€í™˜\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9ï¸âƒ£ ë¬¸ì„œ ìŠ¤í”Œë¦¬íŒ…\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # âœ… ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ì‚¬ìš©\n",
    "\n",
    "    # ðŸ”Ÿ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìž„ë² ë”© ìƒì„±\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # âœ… ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # âœ… ë°°ì—´ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£1ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒì´ë©´ ì´ˆê¸°í™”)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU ì¸ë±ìŠ¤\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPUë¡œ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ì— ë°ì´í„° ì¶”ê°€\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "    \n",
    "    print(f\"âœ… {file} ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ ë¬¸ì„œ ì €ìž¥\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"ðŸŽ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ìž¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¼ë°˜ ë³´í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (GPUë¡œ ì´ë™)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3ï¸âƒ£ ìž„ë² ë”© ìƒì„± í•¨ìˆ˜\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # íŽ˜ì´ì§€ ë²ˆí˜¸ ì œê±°\n",
    "    text = re.sub(r\"(ì œìž‘ì¼|ì£¼ì†Œ|QRì½”ë“œ|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # ëª©ì°¨ ì œê±°\n",
    "    text = re.sub(r\"ê¸ˆì†Œë²•|ë²•ë ¹\", \"\", text)  # ë²•ì  ê³µì§€ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # ê³µë°± ì •ë¦¬\n",
    "    return text.strip()\n",
    "\n",
    "# 4ï¸âƒ£ ì €ìž¥ ê²½ë¡œ ì„¤ì •\n",
    "faiss_index_path = \"./faiss_index_lotte_short.bin\"\n",
    "metadata_path = \"./documents_lotte_short.pkl\"\n",
    "\n",
    "# 6ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„±\n",
    "res = faiss.StandardGpuResources()  # GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in short_list:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDFì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # âœ… ì „ì²´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ì €ìž¥\n",
    "\n",
    "    # 7ï¸âƒ£ í…ìŠ¤íŠ¸ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°\n",
    "    total_text = \"\\n\".join(all_texts)  # âœ… í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ ë³‘í•©\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # âœ… ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n",
    "\n",
    "    # 8ï¸âƒ£ ìƒˆë¡œìš´ íŽ˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€í•˜ë©° Document í˜•ì‹ ë³€í™˜\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9ï¸âƒ£ ë¬¸ì„œ ìŠ¤í”Œë¦¬íŒ…\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # âœ… ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ì‚¬ìš©\n",
    "\n",
    "    # ðŸ”Ÿ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìž„ë² ë”© ìƒì„±\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # âœ… ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # âœ… ë°°ì—´ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£1ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒì´ë©´ ì´ˆê¸°í™”)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU ì¸ë±ìŠ¤\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPUë¡œ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ì— ë°ì´í„° ì¶”ê°€\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "    print(f\"âœ… {file} ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "# 1ï¸âƒ£3ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ì €ìž¥\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ ë¬¸ì„œ ì €ìž¥\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìž¥ê¸° - ìƒí•´, ì§ˆë³‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (GPUë¡œ ì´ë™)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3ï¸âƒ£ ìž„ë² ë”© ìƒì„± í•¨ìˆ˜\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # íŽ˜ì´ì§€ ë²ˆí˜¸ ì œê±°\n",
    "    text = re.sub(r\"(ì œìž‘ì¼|ì£¼ì†Œ|QRì½”ë“œ|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # ëª©ì°¨ ì œê±°\n",
    "    text = re.sub(r\"ê¸ˆì†Œë²•|ë²•ë ¹\", \"\", text)  # ë²•ì  ê³µì§€ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # ê³µë°± ì •ë¦¬\n",
    "    return text.strip()\n",
    "\n",
    "# 4ï¸âƒ£ ì €ìž¥ ê²½ë¡œ ì„¤ì •\n",
    "faiss_index_path = \"./faiss_index_lotte_long_hurt.bin\"\n",
    "metadata_path = \"./documents_lotte_long_hurt.pkl\"\n",
    "\n",
    "# 6ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„±\n",
    "res = faiss.StandardGpuResources()  # GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_hurt:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDFì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # âœ… ì „ì²´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ì €ìž¥\n",
    "\n",
    "    # 7ï¸âƒ£ í…ìŠ¤íŠ¸ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°\n",
    "    total_text = \"\\n\".join(all_texts)  # âœ… í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ ë³‘í•©\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # âœ… ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n",
    "\n",
    "    # 8ï¸âƒ£ ìƒˆë¡œìš´ íŽ˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€í•˜ë©° Document í˜•ì‹ ë³€í™˜\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9ï¸âƒ£ ë¬¸ì„œ ìŠ¤í”Œë¦¬íŒ…\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # âœ… ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ì‚¬ìš©\n",
    "\n",
    "    # ðŸ”Ÿ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìž„ë² ë”© ìƒì„±\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # âœ… ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # âœ… ë°°ì—´ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£1ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒì´ë©´ ì´ˆê¸°í™”)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU ì¸ë±ìŠ¤\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPUë¡œ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ì— ë°ì´í„° ì¶”ê°€\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "        \n",
    "    print(f\"âœ… {file} ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ ë¬¸ì„œ ì €ìž¥\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"ðŸŽ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ìž¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìž¥ê¸° - ì—°ê¸ˆ, ì €ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì—°ê¸ˆë³´í—˜_let care ì—°ê¸ˆì €ì¶•ì†í•´ë³´í—˜â… (2401).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—°ê¸ˆë³´í—˜_let care ì—°ê¸ˆì €ì¶•ì†í•´ë³´í—˜â…¡(2401).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "âœ… ì—°ê¸ˆë³´í—˜_let care ì—°ê¸ˆì €ì¶•ì†í•´ë³´í—˜(ê³„ì•½ì´ì „)(2401).pdf ì²˜ë¦¬ ì™„ë£Œ\n",
      "ðŸŽ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ìž¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# 2ï¸âƒ£ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (GPUë¡œ ì´ë™)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3ï¸âƒ£ ìž„ë² ë”© ìƒì„± í•¨ìˆ˜\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # íŽ˜ì´ì§€ ë²ˆí˜¸ ì œê±°\n",
    "    text = re.sub(r\"(ì œìž‘ì¼|ì£¼ì†Œ|QRì½”ë“œ|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # ëª©ì°¨ ì œê±°\n",
    "    text = re.sub(r\"ê¸ˆì†Œë²•|ë²•ë ¹\", \"\", text)  # ë²•ì  ê³µì§€ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # ê³µë°± ì •ë¦¬\n",
    "    return text.strip()\n",
    "\n",
    "# 4ï¸âƒ£ ì €ìž¥ ê²½ë¡œ ì„¤ì •\n",
    "faiss_index_path = \"./faiss_index_lotte_long_save.bin\"\n",
    "metadata_path = \"./documents_lotte_long_save.pkl\"\n",
    "\n",
    "# 6ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„±\n",
    "res = faiss.StandardGpuResources()  # GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_save:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDFì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # âœ… ì „ì²´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ì €ìž¥\n",
    "\n",
    "    # 7ï¸âƒ£ í…ìŠ¤íŠ¸ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°\n",
    "    total_text = \"\\n\".join(all_texts)  # âœ… í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ ë³‘í•©\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # âœ… ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n",
    "\n",
    "    # 8ï¸âƒ£ ìƒˆë¡œìš´ íŽ˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€í•˜ë©° Document í˜•ì‹ ë³€í™˜\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9ï¸âƒ£ ë¬¸ì„œ ìŠ¤í”Œë¦¬íŒ…\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # âœ… ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ì‚¬ìš©\n",
    "\n",
    "    # ðŸ”Ÿ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìž„ë² ë”© ìƒì„±\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # âœ… ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # âœ… ë°°ì—´ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£1ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒì´ë©´ ì´ˆê¸°í™”)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU ì¸ë±ìŠ¤\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPUë¡œ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ì— ë°ì´í„° ì¶”ê°€\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)    \n",
    "    print(f\"âœ… {file} ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ ë¬¸ì„œ ì €ìž¥\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"ðŸŽ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ìž¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìž¥ê¸° - ê¸°íƒ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (GPUë¡œ ì´ë™)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3ï¸âƒ£ ìž„ë² ë”© ìƒì„± í•¨ìˆ˜\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # íŽ˜ì´ì§€ ë²ˆí˜¸ ì œê±°\n",
    "    text = re.sub(r\"(ì œìž‘ì¼|ì£¼ì†Œ|QRì½”ë“œ|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # ëª©ì°¨ ì œê±°\n",
    "    text = re.sub(r\"ê¸ˆì†Œë²•|ë²•ë ¹\", \"\", text)  # ë²•ì  ê³µì§€ì‚¬í•­ ì œê±°\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # ê³µë°± ì •ë¦¬\n",
    "    return text.strip()\n",
    "\n",
    "# 4ï¸âƒ£ ì €ìž¥ ê²½ë¡œ ì„¤ì •\n",
    "faiss_index_path = \"./faiss_index_lotte_long_etc.bin\"\n",
    "metadata_path = \"./documents_lotte_long_etc.pkl\"\n",
    "\n",
    "# 6ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„±\n",
    "res = faiss.StandardGpuResources()  # GPU ë¦¬ì†ŒìŠ¤ í• ë‹¹\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_etc:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "    except:\n",
    "        documents = []\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            metadata = pdf.metadata  # PDFì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "\n",
    "            for page_num, page in enumerate(pdf):\n",
    "                page_text = page.get_text()\n",
    "                if \"MuPDF error:\" not in page_text:\n",
    "                    documents.append(Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            'producer': metadata.get('producer', ''),\n",
    "                            'creator': metadata.get('creator', ''),\n",
    "                            'creationdate': metadata.get('creationDate', ''),\n",
    "                            'title': metadata.get('title', ''),\n",
    "                            'author': metadata.get('author', ''),\n",
    "                            'moddate': metadata.get('modDate', ''),\n",
    "                            'pdfversion': metadata.get('pdfVersion', ''),\n",
    "                            'source': file_path,\n",
    "                            'total_pages': pdf.page_count,\n",
    "                            'page': page_num,\n",
    "                            'page_label': str(page_num + 1)\n",
    "                        }\n",
    "                    ))\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # âœ… ì „ì²´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ì €ìž¥\n",
    "\n",
    "    # 7ï¸âƒ£ í…ìŠ¤íŠ¸ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°\n",
    "    total_text = \"\\n\".join(all_texts)  # âœ… í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ ë³‘í•©\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # âœ… ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n",
    "\n",
    "    # 8ï¸âƒ£ ìƒˆë¡œìš´ íŽ˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€í•˜ë©° Document í˜•ì‹ ë³€í™˜\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9ï¸âƒ£ ë¬¸ì„œ ìŠ¤í”Œë¦¬íŒ…\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # âœ… ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ì‚¬ìš©\n",
    "\n",
    "    # ðŸ”Ÿ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìž„ë² ë”© ìƒì„±\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # âœ… ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # âœ… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # âœ… ë°°ì—´ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£1ï¸âƒ£ FAISS GPU ì¸ë±ìŠ¤ ìƒì„± (ì²˜ìŒì´ë©´ ì´ˆê¸°í™”)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU ì¸ë±ìŠ¤\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPUë¡œ ë³€í™˜\n",
    "\n",
    "    # 1ï¸âƒ£2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ì— ë°ì´í„° ì¶”ê°€\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)    \n",
    "    print(f\"âœ… {file} ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1ï¸âƒ£4ï¸âƒ£ ë¬¸ì„œ ì €ìž¥\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"ðŸŽ‰ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ìž¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1ï¸âƒ£ ì €ìž¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "index = faiss.read_index(\"./faiss_index.bin\")\n",
    "\n",
    "# 2ï¸âƒ£ ë¬¸ì„œ ì •ë³´ ë¡œë“œ\n",
    "with open(\"./documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# 3ï¸âƒ£ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "# query = \" ë³´í—˜ê¸ˆì˜ ì¢…ë¥˜ ë° í•œë„ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜?\"\n",
    "query = \"ìžë™ì°¨ ì˜ë¬´ë³´í—˜ ë¯¸ê°€ìž…ì— ë”°ë¥¸ ë¶ˆì´ìµì„ ì•Œë ¤ì¤˜\"\n",
    "query_embedding = get_embeddings([query])[0]  # ì¿¼ë¦¬ ìž„ë² ë”©\n",
    "query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "\n",
    "D, I = index.search(query_embedding, k=10)  # ê°€ìž¥ ìœ ì‚¬í•œ 5ê°œ ê²€ìƒ‰\n",
    "context = []\n",
    "\n",
    "# 4ï¸âƒ£ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥\n",
    "for idx in I[0]:\n",
    "    context.append(documents[idx].page_content)\n",
    "    print(f\"ðŸ”¹ ë¬¸ì„œ {idx}: {documents[idx].page_content[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (faiss_2)",
   "language": "python",
   "name": "faiss_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

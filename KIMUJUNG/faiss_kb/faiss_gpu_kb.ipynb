{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
      "Collecting frontend\n",
      "  Downloading frontend-0.0.3-py3-none-any.whl.metadata (847 bytes)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting configobj (from fitz)\n",
      "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting configparser (from fitz)\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: httplib2 in /usr/lib/python3/dist-packages (from fitz) (0.20.2)\n",
      "Collecting nibabel (from fitz)\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting nipype (from fitz)\n",
      "  Downloading nipype-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.24.1)\n",
      "Collecting pandas (from fitz)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyxnat (from fitz)\n",
      "  Downloading pyxnat-1.6.3-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scipy (from fitz)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette>=0.12.0 (from frontend)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting uvicorn>=0.7.1 (from frontend)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting itsdangerous>=1.1.0 (from frontend)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting aiofiles (from frontend)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.4.0)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.45 (from langchain)\n",
      "  Downloading langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.19-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy (from fitz)\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.45->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing_extensions>=4.0 (from pypdf)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.12.0->frontend) (4.0.0)\n",
      "Collecting click>=7.0 (from uvicorn>=0.7.1->frontend)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.7.1->frontend)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/lib/python3/dist-packages (from httplib2->fitz) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting importlib-resources>=5.12 (from nibabel->fitz)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting prov>=1.5.2 (from nipype->fitz)\n",
      "  Downloading prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pydot>=1.2.3 (from nipype->fitz)\n",
      "  Downloading pydot-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
      "  Downloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting traits>=6.2 (from nipype->fitz)\n",
      "  Downloading traits-7.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting acres (from nipype->fitz)\n",
      "  Downloading acres-0.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting etelemetry>=0.3.1 (from nipype->fitz)\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting looseversion!=1.2 (from nipype->fitz)\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting puremagic (from nipype->fitz)\n",
      "  Downloading puremagic-1.28-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->fitz)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->fitz)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.3)\n",
      "Collecting pathlib>=1.0 (from pyxnat->fitz)\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.1.3)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.3.1->nipype->fitz)\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (2.4)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2->fitz)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m162.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m209.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m203.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m268.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m252.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m172.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.19-py3-none-any.whl (351 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.9/351.9 kB\u001b[0m \u001b[31m148.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m282.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m153.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m165.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m274.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m229.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m186.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m260.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m260.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
      "Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m194.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nipype-1.10.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m202.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m251.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyxnat-1.6.3-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m182.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m156.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydot-3.0.4-py3-none-any.whl (35 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m176.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading traits-7.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m198.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m128.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m208.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading acres-0.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pytz, puremagic, pathlib, looseversion, zstandard, xxhash, tzdata, typing_extensions, traits, tqdm, tenacity, simplejson, safetensors, requests, regex, python-dotenv, pyparsing, pymupdf, pyarrow, propcache, orjson, numpy, mypy-extensions, marshmallow, jsonpatch, itsdangerous, isodate, importlib-resources, httpx-sse, h11, greenlet, fsspec, frozenlist, dill, configparser, configobj, click, ci-info, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, uvicorn, typing-inspection, typing-inspect, starlette, SQLAlchemy, scipy, requests-toolbelt, rdflib, pyxnat, pypdf, pydot, pydantic-core, pandas, nibabel, multiprocess, multidict, huggingface-hub, httpcore, etelemetry, dotenv, aiosignal, acres, yarl, tokenizers, pydantic, prov, httpx, frontend, dataclasses-json, transformers, pydantic-settings, nipype, langsmith, aiohttp, langchain-core, fitz, langchain-text-splitters, datasets, langchain, langchain_community\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed SQLAlchemy-2.0.40 acres-0.3.0 aiofiles-24.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-4.0.3 ci-info-0.3.0 click-8.1.8 configobj-5.0.9 configparser-7.2.0 dataclasses-json-0.6.7 datasets-3.5.0 dill-0.3.8 dotenv-0.9.9 etelemetry-0.3.1 fitz-0.0.1.dev2 frontend-0.0.3 frozenlist-1.5.0 fsspec-2024.12.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.29.3 importlib-resources-6.5.2 isodate-0.6.1 itsdangerous-2.2.0 jsonpatch-1.33 langchain-0.3.21 langchain-core-0.3.49 langchain-text-splitters-0.3.7 langchain_community-0.3.20 langsmith-0.3.19 looseversion-1.3.0 marshmallow-3.26.1 multidict-6.2.0 multiprocess-0.70.16 mypy-extensions-1.0.0 nibabel-5.3.2 nipype-1.10.0 numpy-2.2.4 orjson-3.10.16 pandas-2.2.3 pathlib-1.0.1 propcache-0.3.1 prov-2.0.1 puremagic-1.28 pyarrow-19.0.1 pydantic-2.11.1 pydantic-core-2.33.0 pydantic-settings-2.8.1 pydot-3.0.4 pymupdf-1.25.4 pyparsing-3.2.3 pypdf-5.4.0 python-dotenv-1.1.0 pytz-2025.2 pyxnat-1.6.3 rdflib-6.3.2 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 safetensors-0.5.3 scipy-1.15.2 simplejson-3.20.1 starlette-0.46.1 tenacity-9.0.0 tokenizers-0.21.1 tqdm-4.67.1 traits-7.0.2 transformers-4.50.3 typing-inspect-0.9.0 typing-inspection-0.4.0 typing_extensions-4.13.0 tzdata-2025.2 uvicorn-0.34.0 xxhash-3.5.0 yarl-1.18.3 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m176.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.4\n",
      "    Uninstalling numpy-2.2.4:\n",
      "      Successfully uninstalled numpy-2.2.4\n",
      "Successfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting pybind11\n",
      "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pybind11\n",
      "Successfully installed pybind11-2.13.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fitz frontend pymupdf pypdf dotenv langchain torch langchain_community transformers datasets\n",
    "!pip install \"numpy<2\"\n",
    "!pip install --upgrade pybind11\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    " \n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 1️⃣ GPU 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동차 보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb8910d9b31492d9df3d9a46344102c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf24821b0f4e78ae22d028e6c0cc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1ee55579644dd8843791ab6feb1400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada8845149904f9aac4e17bc1db08e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc750a443c2498588e6a0b38b90b06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_kanana2vec.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- configuration_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8fd8f9191042ffb7ee6e1f03cce90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_kanana2vec.py:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- modeling_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e366dd3c48974b9eaf009d47333c31c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 4_250228_privatePM.pdf 처리 완료\n",
      "✅ 4_250228_privateCM.pdf 처리 완료\n",
      "✅ 10_250228_businessCM.pdf 처리 완료\n",
      "✅ 4_250228_privateTM[0].pdf 처리 완료\n",
      "✅ 10_250228_businessTM.pdf 처리 완료\n",
      "✅ 15_250306_commercialCM.pdf 처리 완료\n",
      "✅ 23_240418_Driver-ServiceCM[0].pdf 처리 완료\n",
      "✅ 13_240906_comprehensive.pdf 처리 완료\n",
      "✅ 14_240906_comprehensiveTM.pdf 처리 완료\n",
      "✅ 4_250228_private.pdf 처리 완료\n",
      "✅ 공동약관_개인용_250101[1].pdf 처리 완료\n",
      "✅ 공동약관_업무용_240906.pdf 처리 완료\n",
      "✅ 공동약관_영업용_240906.pdf 처리 완료\n",
      "✅ 공동약관_이륜차_230401[0].pdf 처리 완료\n",
      "✅ 10_250228_business.pdf 처리 완료\n",
      "✅ 25_240131_PlatformDelivery.pdf 처리 완료\n",
      "✅ 17_230401_Two-WheelⅡ.pdf 처리 완료\n",
      "✅ 19_250221_Two-WheelCM.pdf 처리 완료\n",
      "✅ 34_241201_KB배달라이더이륜자동차보험.pdf 처리 완료\n",
      "✅ 29_231115_Electromobile.pdf 처리 완료\n",
      "✅ 26_231115_Crackdown.pdf 처리 완료\n",
      "✅ 15_250306_commercial.pdf 처리 완료\n",
      "✅ 15_250306_commercialTM.pdf 처리 완료\n",
      "✅ 19_231115_Handle.pdf 처리 완료\n",
      "✅ 20_250221_Two-WheelTM.pdf 처리 완료\n",
      "✅ 27_231115_License.pdf 처리 완료\n",
      "✅ 22_240418_Driver-Service.pdf 처리 완료\n",
      "✅ 22_231115_Haru.pdf 처리 완료\n",
      "✅ 18-250221_Two-Wheel.pdf 처리 완료\n",
      "✅ 30.231115_단기이륜차운전자.pdf 처리 완료\n",
      "✅ 24_231115_Motorbike.pdf 처리 완료\n",
      "✅ 28_231115_Driver.pdf 처리 완료\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "path = \"pdf_folder_kb/자동차보험\"\n",
    "file_list = os.listdir(path)\n",
    "car_list = os.listdir(\"pdf_folder_kb/자동차보험\")\n",
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_kb_car.bin\"\n",
    "metadata_path = \"./documents_kb_car.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in car_list:\n",
    "    all_texts = []\n",
    "    file_path = os.path.join(path, file)\n",
    "    \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "    \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반 보험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_list = os.listdir(\"pdf_folder_kb/일반보험\") + os.listdir(\"pdf_folder_kb/운전자보험\")\n",
    "len(short_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3c672e10cb4d98aed81225be1550d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a1f248ca354b00b6716f0f7a6e6793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317ee36ff2844d5ba9b3d440d037856b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f9001dec0245b9a69255cb170d20d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae3e411ecea4a3bbbd710a64cf7f57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_kanana2vec.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- configuration_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41ae58bedf7474f84ceab10381b676f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_kanana2vec.py:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- modeling_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0e2e879b3144458840a5ac36f31b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 20250101_17363_1.pdf 처리 완료\n",
      "✅ 20250101_17385_1.pdf 처리 완료\n",
      "✅ 20250101_17421_1.pdf 처리 완료\n",
      "✅ 20240401_17328_1.pdf 처리 완료\n",
      "✅ 20250101_17388_1.pdf 처리 완료\n",
      "✅ 20250101_17365_1.pdf 처리 완료\n",
      "✅ 20250101_17101_1.pdf 처리 완료\n",
      "✅ 20240401_17396_1.pdf 처리 완료\n",
      "✅ 20250101_17364_1.pdf 처리 완료\n",
      "✅ 20240401_17401_1.pdf 처리 완료\n",
      "✅ 20250101_16121_1.pdf 처리 완료\n",
      "✅ 20250101_16240_1.pdf 처리 완료\n",
      "✅ 20240531_17305_1.pdf 처리 완료\n",
      "✅ 20250101_10105_1.pdf 처리 완료\n",
      "✅ 20250101_15317_1.pdf 처리 완료\n",
      "✅ 20250101_15336_1.pdf 처리 완료\n",
      "✅ 20250101_15125_1.pdf 처리 완료\n",
      "✅ 20250101_15325_1.pdf 처리 완료\n",
      "✅ 20240401_17391_1.pdf 처리 완료\n",
      "✅ 20250101_17390_1.pdf 처리 완료\n",
      "✅ 20250101_10101_1.pdf 처리 완료\n",
      "✅ 20250101_17309_1.pdf 처리 완료\n",
      "✅ 20250101_15506_1.pdf 처리 완료\n",
      "✅ 20250101_10106_1.pdf 처리 완료\n",
      "✅ 20250101_10102_1.pdf 처리 완료\n",
      "✅ 20250101_10108_1.pdf 처리 완료\n",
      "✅ 20250101_15105_1.pdf 처리 완료\n",
      "✅ 20250101_15332_1.pdf 처리 완료\n",
      "✅ 20250101_16119_1.pdf 처리 완료\n",
      "✅ 20250101_15116_1.pdf 처리 완료\n",
      "✅ 20250101_15337_1.pdf 처리 완료\n",
      "✅ 20250101_15235_1.pdf 처리 완료\n",
      "✅ 20250101_15104_1.pdf 처리 완료\n",
      "✅ 20250101_16101_1.pdf 처리 완료\n",
      "✅ 20250101_15122_1.pdf 처리 완료\n",
      "✅ 20250201_15237_1.pdf 처리 완료\n",
      "✅ 20250205_15239_1.pdf 처리 완료\n",
      "✅ 20250101_15120_1.pdf 처리 완료\n",
      "✅ 20250101_15234_1.pdf 처리 완료\n",
      "✅ 20250301_15226_1.pdf 처리 완료\n",
      "✅ 20250101_14171_1.pdf 처리 완료\n",
      "✅ 20250101_15223_1.pdf 처리 완료\n",
      "✅ 20250101_15313_1.pdf 처리 완료\n",
      "✅ 20250301_15238_1.pdf 처리 완료\n",
      "✅ 20250101_15301_1.pdf 처리 완료\n",
      "✅ 20250101_16116_1.pdf 처리 완료\n",
      "✅ 20250101_14178_1.pdf 처리 완료\n",
      "✅ 20250301_15224_1.pdf 처리 완료\n",
      "✅ 20250101_15322_1_1.pdf 처리 완료\n",
      "✅ 20250101_15324_1.pdf 처리 완료\n",
      "✅ 20250101_15101_1.pdf 처리 완료\n",
      "✅ 20250101_15242_1.pdf 처리 완료\n",
      "✅ 20250212_17431_1.pdf 처리 완료\n",
      "✅ 20250301_15202_1.pdf 처리 완료\n",
      "✅ 20250101_15241_1.pdf 처리 완료\n",
      "✅ 20250101_15307_1.pdf 처리 완료\n",
      "✅ 20250101_15333_1.pdf 처리 완료\n",
      "✅ 20250301_15219_1.pdf 처리 완료\n",
      "✅ 24670_1_1.pdf 처리 완료\n"
     ]
    }
   ],
   "source": [
    "# short_list = os.listdir(\"pdf_folder_kb/일반보험\") + os.listdir(\"pdf_folder_kb/운전자보험\")\n",
    "\n",
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_kb_short.bin\"\n",
    "metadata_path = \"./documents_kb_short.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in short_list:\n",
    "    all_texts = []\n",
    "    if file in os.listdir(\"pdf_folder_kb/일반보험\"):\n",
    "        path = \"pdf_folder_kb/일반보험\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/운전자보험\"):\n",
    "        path = \"pdf_folder_kb/운전자보험\"\n",
    "\n",
    "    file_path = os.path.join(path, file)\n",
    "    \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "        \n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "# 1️⃣3️⃣ FAISS 인덱스 저장\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장기 - 상해, 질병"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed3a308bf984aaab693b08e0bf44382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e46df48a6724e35acb878735a7deb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d580f55fa5f4c3184903925e5aaf86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c192e2be16492e8cf657f71c24b61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea2d7b4a3ff4481b811c1116e9cc2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_kanana2vec.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- configuration_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdc38bbedd34c03bcacf047835f4271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_kanana2vec.py:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- modeling_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de0c47ed05148548d1cd47863f505cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 24611_3_1(일반).pdf 처리 완료\n",
      "✅ 24609_3_1.pdf 처리 완료\n",
      "✅ 24608_3_1.pdf 처리 완료\n",
      "✅ 24605_3_1.pdf 처리 완료\n",
      "✅ 24606_3_1.pdf 처리 완료\n",
      "✅ 24603_3_1.pdf 처리 완료\n",
      "✅ 24602_3_1.pdf 처리 완료\n",
      "✅ 24600_3_1.pdf 처리 완료\n",
      "✅ 24599_3_1[0].pdf 처리 완료\n",
      "✅ 24661_1_1.pdf 처리 완료\n",
      "✅ 24662_1_1.pdf 처리 완료\n",
      "✅ 24660_1_1.Pdf 처리 완료\n",
      "✅ 24666_1_1.pdf 처리 완료\n",
      "✅ 24629_1_1(일반).pdf 처리 완료\n",
      "✅ 24685_1_1.pdf 처리 완료\n",
      "✅ 24665_1_1.pdf 처리 완료\n",
      "✅ 24629_1_1(간편)[0].pdf 처리 완료\n",
      "✅ 24639_1_1.pdf 처리 완료\n",
      "✅ 24640_1_1.pdf 처리 완료\n",
      "✅ 24635_1_1.pdf 처리 완료\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "long_hurt = os.listdir(\"pdf_folder_kb/상해보험\") + os.listdir(\"pdf_folder_kb/질병보험\") + os.listdir(\"pdf_folder_kb/방카슈랑스\")\n",
    "\n",
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_kb_long_hurt.bin\"\n",
    "metadata_path = \"./documents_kb_long_hurt.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_hurt:\n",
    "    all_texts = []\n",
    "    if file in os.listdir(\"pdf_folder_kb/상해보험\"):\n",
    "        path = \"pdf_folder_kb/상해보험\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/질병보험\"):\n",
    "        path = \"pdf_folder_kb/질병보험\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/방카슈랑스\"):\n",
    "        path = \"pdf_folder_kb/방카슈랑스\"\n",
    "\n",
    "    file_path = os.path.join(path, file)\n",
    "    \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "        \n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)\n",
    "        \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장기 - 연금, 저축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e9411067424e3681d8436593a2a4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534545a3445e4c1795277d8e49dd7d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca731fe7cfe45ea9a63175609265663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f397fbf6834899ad6f2359f506cc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1796e3192a4ef3b7761d08e4878792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_kanana2vec.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- configuration_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b227238bf646402cb33913c49605d69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_kanana2vec.py:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- modeling_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9d16c2a19d49118afbba86ecca8850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 24632_1_1.pdf 처리 완료\n",
      "✅ 24633_1_1.pdf 처리 완료\n",
      "✅ 24634_1_1.pdf 처리 완료\n",
      "✅ dc-pension(241213).pdf 처리 완료\n",
      "✅ irp(p)-terms(241213).pdf 처리 완료\n",
      "✅ irp(c)-pension(241213).pdf 처리 완료\n",
      "✅ db-pension(241213).pdf 처리 완료\n",
      "✅ irp(c)-terms(241213).pdf 처리 완료\n",
      "✅ db-terms(241213).pdf 처리 완료\n",
      "✅ irp(p)-pension(241213).pdf 처리 완료\n",
      "✅ gic-terms(241213).pdf 처리 완료\n",
      "✅ dc-terms(241213).pdf 처리 완료\n",
      "✅ yct-terms(241213).pdf 처리 완료\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "long_save = os.listdir(\"pdf_folder_kb/저축성보험\") + os.listdir(\"pdf_folder_kb/개인연금\") + os.listdir(\"pdf_folder_kb/퇴직연금\")\n",
    "\n",
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_kb_long_save.bin\"\n",
    "metadata_path = \"./documents_kb_long_save.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_save:\n",
    "    all_texts = []\n",
    "    if file in os.listdir(\"pdf_folder_kb/저축성보험\"):\n",
    "        path = \"pdf_folder_kb/저축성보험\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/개인연금\"):\n",
    "        path = \"pdf_folder_kb/개인연금\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/퇴직연금\"):\n",
    "        path = \"pdf_folder_kb/퇴직연금\"\n",
    "\n",
    "    file_path = os.path.join(path, file)\n",
    "   \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)    \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장기 - 기타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8dbdbfb7c9436e812081dea6af09e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a85d6a62af4a5ebfef2eb80a211296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d074529fe4f45038f3a496640a1bb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c08edcd9b55417aa9e3ee82ca7cf2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864914197cc14c579d9eaaa8f0a1fccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_kanana2vec.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- configuration_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcccf14d006342ebb6c3b7f1d6b1127f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_kanana2vec.py:   0%|          | 0.00/9.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding:\n",
      "- modeling_kanana2vec.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83eae2f561c54b6193055080b25e50fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 24693_1_1.pdf 처리 완료\n",
      "✅ 24664_1_1.pdf 처리 완료\n",
      "✅ 24663_1_1.pdf 처리 완료\n",
      "✅ 24657_1_1.pdf 처리 완료\n",
      "✅ 24655_1_1.pdf 처리 완료\n",
      "✅ 24650_1_1.pdf 처리 완료\n",
      "✅ 24656_1_1.pdf 처리 완료\n",
      "✅ 24628_1_1[0].pdf 처리 완료\n",
      "✅ 24625_3_1(일반).pdf 처리 완료\n",
      "✅ 24471_2_1.pdf 처리 완료\n",
      "✅ 24649_1_1.pdf 처리 완료\n",
      "✅ 24648_1_1.pdf 처리 완료\n",
      "✅ 24626_3_1(일반).pdf 처리 완료\n",
      "✅ 24622_3_1(일반).pdf 처리 완료\n",
      "✅ 24621_3_1(일반).pdf 처리 완료\n",
      "✅ 24618_1_1(일반).pdf 처리 완료\n",
      "✅ 24613_3_1(일반).pdf 처리 완료\n",
      "✅ 24690_1_1.pdf 처리 완료\n",
      "✅ 24607_3_1.pdf 처리 완료\n",
      "✅ 24698_1_1.pdf 처리 완료\n",
      "✅ 24675_1_1.pdf 처리 완료\n",
      "✅ 24674_1_1.pdf 처리 완료\n",
      "✅ 24687_1_1.Pdf 처리 완료\n",
      "✅ 24688_1_1.pdf 처리 완료\n",
      "✅ 24712_1_1.pdf 처리 완료\n",
      "✅ 24689_1_1.pdf 처리 완료\n",
      "✅ 24601_3_1.pdf 처리 완료\n",
      "✅ 24708_1_1.pdf 처리 완료\n",
      "✅ 24684_1_1.pdf 처리 완료\n",
      "✅ 24638_1_1.pdf 처리 완료\n",
      "✅ 24691_1_1.pdf 처리 완료\n",
      "✅ 24709_1_1.pdf 처리 완료\n",
      "✅ 24696_1_1.pdf 처리 완료\n",
      "✅ 24678_1_1.pdf 처리 완료\n",
      "✅ 24697_1_1.pdf 처리 완료\n",
      "✅ 24699_1_1.pdf 처리 완료\n",
      "✅ 24682_1_1.pdf 처리 완료\n",
      "✅ 24671_1_1.pdf 처리 완료\n",
      "✅ 24647_1_1.pdf 처리 완료\n",
      "✅ 24478_1_1.pdf 처리 완료\n",
      "✅ 24604_3_1.pdf 처리 완료\n",
      "✅ 24704_1_1.pdf 처리 완료\n",
      "✅ 24680_1_1.pdf 처리 완료\n",
      "✅ 24692_1_1.pdf 처리 완료\n",
      "✅ 24707_1_1.pdf 처리 완료\n",
      "✅ 24701_1_1.pdf 처리 완료\n",
      "✅ 24637_1_1.pdf 처리 완료\n",
      "✅ 24630_1_1(일반).pdf 처리 완료\n",
      "✅ 0000_1_1.pdf 처리 완료\n",
      "✅ 0072_1_1.pdf 처리 완료\n",
      "✅ 0073_1_1.pdf 처리 완료\n",
      "✅ 0074_1_1.pdf 처리 완료\n",
      "✅ 0037_4_1.pdf 처리 완료\n",
      "✅ 0056_1_1.Pdf 처리 완료\n",
      "✅ 0058_1_1.pdf 처리 완료\n",
      "✅ 0079_1_1.pdf 처리 완료\n",
      "✅ 0034_3_1.pdf 처리 완료\n",
      "✅ 0055_1_1.pdf 처리 완료\n",
      "✅ 0078_1_1.pdf 처리 완료\n",
      "✅ 0075_1_1.pdf 처리 완료\n",
      "✅ 0062_2_1.pdf 처리 완료\n",
      "✅ 0077_1_1.pdf 처리 완료\n",
      "✅ 0029_1_1.pdf 처리 완료\n",
      "✅ 0076_1_1.pdf 처리 완료\n",
      "✅ 0057_2_1.pdf 처리 완료\n",
      "✅ 0054_1_1.pdf 처리 완료\n",
      "✅ 0061_1_1.pdf 처리 완료\n",
      "✅ 0047_4_1.pdf 처리 완료\n",
      "✅ 0011_4_1.pdf 처리 완료\n",
      "✅ 0067_1_1.pdf 처리 완료\n",
      "✅ 0044_6_1.pdf 처리 완료\n",
      "✅ 0059_1_1.pdf 처리 완료\n",
      "✅ 0050_5_1.pdf 처리 완료\n",
      "✅ 0060_1_1[0].pdf 처리 완료\n",
      "✅ 0068_1_1.pdf 처리 완료\n",
      "✅ 0064_1_1.pdf 처리 완료\n",
      "✅ 0066_1_1.pdf 처리 완료\n",
      "✅ 0036_2_1.pdf 처리 완료\n",
      "✅ 0002_1_1.pdf 처리 완료\n",
      "✅ 0009_1_1.pdf 처리 완료\n",
      "✅ 0003_1_1.pdf 처리 완료\n",
      "✅ 0024_1_1.pdf 처리 완료\n",
      "✅ 0001_1_1.pdf 처리 완료\n",
      "✅ 0065_1_1.pdf 처리 완료\n",
      "✅ 24703_1_1.pdf 처리 완료\n",
      "✅ 24481_2_1.pdf 처리 완료\n",
      "✅ 24702_1_1.pdf 처리 완료\n",
      "✅ 24706_1_1.pdf 처리 완료\n",
      "✅ 24700_1_1.pdf 처리 완료\n",
      "✅ 24705_1_1.pdf 처리 완료\n",
      "✅ 24686_1_1.pdf 처리 완료\n",
      "🎉 모든 문서 처리 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "long_etc = os.listdir(\"pdf_folder_kb/제휴\") + os.listdir(\"pdf_folder_kb/제도성 특별약관\") + os.listdir(\"pdf_folder_kb/기타\") + os.listdir(\"pdf_folder_kb/화재보험\")\n",
    "\n",
    "# 2️⃣ 모델 및 토크나이저 로드 (GPU로 이동)\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "# 3️⃣ 임베딩 생성 함수\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.cpu().numpy()\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"p\\.\\d+\", \"\", text)  # 페이지 번호 제거\n",
    "    text = re.sub(r\"(제작일|주소|QR코드|MEMO).*?(\\n|$)\", \"\", text, flags=re.DOTALL)  # 목차 제거\n",
    "    text = re.sub(r\"금소법|법령\", \"\", text)  # 법적 공지사항 제거\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)  # 공백 정리\n",
    "    return text.strip()\n",
    "\n",
    "# 4️⃣ 저장 경로 설정\n",
    "faiss_index_path = \"./faiss_index_kb_long_etc.bin\"\n",
    "metadata_path = \"./documents_kb_long_etc.pkl\"\n",
    "\n",
    "# 6️⃣ FAISS GPU 인덱스 생성\n",
    "res = faiss.StandardGpuResources()  # GPU 리소스 할당\n",
    "index = None\n",
    "\n",
    "all_documents = []\n",
    "for file in long_etc:\n",
    "    all_texts = []\n",
    "    if file in os.listdir(\"pdf_folder_kb/제휴\"):\n",
    "        path = \"pdf_folder_kb/제휴\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/제도성 특별약관\"):\n",
    "        path = \"pdf_folder_kb/제도성 특별약관\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/기타\"):\n",
    "        path = \"pdf_folder_kb/기타\"\n",
    "    elif file in os.listdir(\"pdf_folder_kb/화재보험\"):\n",
    "        path = \"pdf_folder_kb/화재보험\"\n",
    "\n",
    "    file_path = os.path.join(path, file)\n",
    "\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"source\"] = file\n",
    "        all_texts.append(doc.page_content)  # ✅ 전체 텍스트 리스트에 저장\n",
    "\n",
    "    # 7️⃣ 텍스트 병합 후 중복 제거\n",
    "    total_text = \"\\n\".join(all_texts)  # ✅ 하나의 문자열로 병합\n",
    "    unique_texts = list(dict.fromkeys(total_text.split(\"\\n\")))  # ✅ 중복 제거 후 리스트 변환\n",
    "\n",
    "    # 8️⃣ 새로운 페이지 번호 추가하며 Document 형식 변환\n",
    "    new_documents = []\n",
    "    for i, text in enumerate(unique_texts):\n",
    "        new_doc = Document(page_content=text, metadata={\"page\": i + 1})\n",
    "        new_documents.append(new_doc)\n",
    "\n",
    "    # 9️⃣ 문서 스플리팅\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=500)\n",
    "    split_documents = splitter.split_documents(new_documents)  # ✅ 중복 제거된 문서 사용\n",
    "\n",
    "    # 🔟 텍스트 추출 및 임베딩 생성\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "    embeddings = []  # ✅ 리스트 초기화\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.append(batch_embeddings)  # ✅ 리스트에 추가\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(np.float32)  # ✅ 배열 변환\n",
    "\n",
    "    # 1️⃣1️⃣ FAISS GPU 인덱스 생성 (처음이면 초기화)\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)  # CPU 인덱스\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # GPU로 변환\n",
    "\n",
    "    # 1️⃣2️⃣ FAISS 인덱스에 데이터 추가\n",
    "    index.add(embeddings)\n",
    "    all_documents.extend(split_documents)    \n",
    "    print(f\"✅ {file} 처리 완료\")\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_path)\n",
    "\n",
    "# 1️⃣4️⃣ 문서 저장\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "print(\"🎉 모든 문서 처리 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 48, 36, 6, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_etc = os.listdir(\"pdf_folder_kb/제휴\") + os.listdir(\"pdf_folder_kb/제도성 특별약관\") + os.listdir(\"pdf_folder_kb/기타\") + os.listdir(\"pdf_folder_kb/화재보험\")\n",
    "len(long_etc), len(os.listdir(\"pdf_folder_kb/제휴\")), len(os.listdir(\"pdf_folder_kb/제도성 특별약관\")), len(os.listdir(\"pdf_folder_kb/기타\")), len(os.listdir(\"pdf_folder_kb/화재보험\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1️⃣ 저장된 FAISS 인덱스 로드\n",
    "index = faiss.read_index(\"./faiss_index.bin\")\n",
    "\n",
    "# 2️⃣ 문서 정보 로드\n",
    "with open(\"./documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# 3️⃣ 검색 수행\n",
    "# query = \" 보험금의 종류 및 한도에 대해 설명해줘?\"\n",
    "query = \"자동차 의무보험 미가입에 따른 불이익을 알려줘\"\n",
    "query_embedding = get_embeddings([query])[0]  # 쿼리 임베딩\n",
    "query_embedding = np.array([query_embedding], dtype=np.float32)\n",
    "\n",
    "D, I = index.search(query_embedding, k=10)  # 가장 유사한 5개 검색\n",
    "context = []\n",
    "\n",
    "# 4️⃣ 검색 결과 출력\n",
    "for idx in I[0]:\n",
    "    context.append(documents[idx].page_content)\n",
    "    print(f\"🔹 문서 {idx}: {documents[idx].page_content[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b8118d",
   "metadata": {},
   "source": [
    "[흐름 요약]\n",
    "1. 사용자 보험사 / 종류 선택 -> 벡터DB 주소 반환\n",
    "2. 사용자 입력문 -> DB로드 -> 검색 -> 프롬프트 생성 -> 모델 답변 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49629d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 1: 보험사와 보험종류로 FAISS/문서 경로 반환\n",
    "def get_db_path(ins_name, ins_type):\n",
    "    \n",
    "    # 사용자가 선택한 보험사와 보험상품명을 기반으로 FAISS 및 문서 경로를 반환한다.\n",
    "    base_dir = \"./faiss_store\"\n",
    "    folder = f\"{ins_name}_{ins_type}\"  # 예: \"hana_car\"\n",
    "    \n",
    "    faiss_path = f\"{base_dir}/{folder}/faiss_index.bin\"\n",
    "    doc_path = f\"{base_dir}/{folder}/documents.pkl\"\n",
    "    \n",
    "    return faiss_path, doc_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f77d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d3afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 2 : 사용자 질문 입력 → LLM 응답 생성 함수\n",
    "def run_llm_query(query_text, faiss_path, meta_path, model_id=\"HuggingFaceH4/zephyr-7b-alpha\"):\n",
    "\n",
    "    # 사용자 질문(query_text)을 벡터DB에서 검색하고, \n",
    "    # 유사 문서 기반 프롬프트를 구성해 Hugging Face LLM 응답을 생성한다.\n",
    "\n",
    "    # 1. 임베딩 모델 로드\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 2. 벡터 DB 및 메타데이터 로드\n",
    "    vector_db = FAISS.load_local(faiss_path, embeddings=embedding_model)\n",
    "    retriever = vector_db.as_retriever()\n",
    "\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "\n",
    "    # 3. 유사 문서 검색\n",
    "    top_docs = retriever.get_relevant_documents(query_text)[:3]\n",
    "\n",
    "    # 4. 프롬프트 구성\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in top_docs])\n",
    "    llm_prompt = f\"\"\"[문서 내용]\n",
    "{context_text}\n",
    "\n",
    "[질문]\n",
    "{query_text}\n",
    "\n",
    "[답변]\n",
    "\"\"\"\n",
    "\n",
    "    # 5. Hugging Face LLM 응답 생성\n",
    "    url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {hf_api_key}\"}\n",
    "    payload = {\"inputs\": llm_prompt, \"parameters\": {\"max_new_tokens\": 300}}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        generated = response.json()[0][\"generated_text\"]\n",
    "        llm_response = generated.replace(llm_prompt, \"\").strip()\n",
    "        return llm_response\n",
    "    else:\n",
    "        return f\"⚠️ 오류 발생: {response.status_code} - {response.text}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

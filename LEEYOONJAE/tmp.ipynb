{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list: ['24599_3_1[0].pdf', '24600_3_1.pdf', '24602_3_1.pdf', '24603_3_1.pdf', '24605_3_1.pdf', '24606_3_1.pdf', '24608_3_1.pdf', '24609_3_1.pdf', '24610_3_1.pdf', '24611_3_1(ì¼ë°˜).pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"./data/ìƒí•´ë³´í—˜\"\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "print (\"file_list: {}\".format(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "model_name = \"kakaocorp/kanana-nano-2.1b-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # pool_mask ìƒì„± (ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë§ì¶°ì„œ 1ë¡œ ì„¤ì •)\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)  # pool_mask ì¶”ê°€\n",
    "\n",
    "    return outputs.embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1ï¸âƒ£ FAISS ì €ì¥ ê²½ë¡œ\n",
    "faiss_index_path = \"./faiss_index_1.bin\"\n",
    "metadata_path = \"./documents_1.pkl\"\n",
    "\n",
    "# 2ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (Sentence-BERT ì‚¬ìš©)\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# ## ë³€í™˜\n",
    "# def get_embeddings(texts):\n",
    "#     \"\"\"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì„ë² ë”©ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "#     return embedding_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    pool_mask = torch.ones(inputs[\"input_ids\"].shape, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, pool_mask=pool_mask)\n",
    "\n",
    "    return outputs.embedding.numpy()\n",
    "\n",
    "\n",
    "# 3ï¸âƒ£ PDF ë¬¸ì„œ ë¡œë“œ\n",
    "documents = []  # ëª¨ë“  ë¬¸ì„œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "for file in file_list:  # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ë¡œë“œ\n",
    "    loader = PyPDFLoader(path + \"\\\\\" + file)\n",
    "    documents.extend(loader.load())  # âœ… ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "\n",
    "# 4ï¸âƒ£ ë¬¸ì„œ ì „ì²˜ë¦¬\n",
    "for doc in documents:\n",
    "    doc.page_content = doc.page_content.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    doc.metadata[\"source\"] = file  # âœ… ë¬¸ì„œì˜ ì¶œì²˜ ì •ë³´ë¥¼ ì¶”ê°€\n",
    "\n",
    "# 5ï¸âƒ£ ë¬¸ì„œ ìŠ¤í”Œë¦¬íŒ…\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "split_documents = splitter.split_documents(documents)  # âœ… ë¦¬ìŠ¤íŠ¸ ì „ì²´ë¥¼ ì…ë ¥í•´ì•¼ í•¨\n",
    "\n",
    "# 6ï¸âƒ£ ìŠ¤í”Œë¦¿ëœ ë¬¸ì„œë¡œ ë²¡í„° ìƒì„±\n",
    "batch_size = 16\n",
    "texts = [doc.page_content for doc in split_documents]\n",
    "embeddings = []\n",
    "# print(texts)\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i : i + batch_size]\n",
    "    batch_embeddings = get_embeddings(batch)  # ğŸ”¥ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "# 7ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥\n",
    "embedding_dim = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(np.array(embeddings, dtype=np.float32))\n",
    "\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(split_documents, f)\n",
    "\n",
    "print(\"âœ… FAISS ì¸ë±ìŠ¤ ë° ë¬¸ì„œ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\n",
      "ğŸ”¹ ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: 6488\n"
     ]
    }
   ],
   "source": [
    "# ì½”ë“œ ì €ì¥ë¬ëŠ”ì§€ í™•ì¸\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# ì €ì¥ëœ ì¸ë±ìŠ¤ & ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "index = faiss.read_index(\"./faiss_index_1.bin\")\n",
    "\n",
    "with open(\"./documents_1.pkl\", \"rb\") as f:\n",
    "    split_documents = pickle.load(f)\n",
    "\n",
    "print(\"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ”¹ ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {len(split_documents)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
